\chapter{Methodology}\label{ch:4}
This chapter provides the general background on the methodology applied in this thesis. First introducing the industry standard for default predictions and this thesis' benchmark model, that is the approach of logistic regression, then the chapter introduces Artificial Neural Networks as and alternative approach for in this case classification of defaults and non-defaults. At last the Local Interpretable Model-agnostic Explanations (LIME) method for explaining >>black-box<< models is introduced.
    \section{Logistic Regression}
    The logistic regression was proposed by \cite{David_Cox_1958} and is a linear classifier, which models the probability of the possible classes in the target variable. In this thesis there are two classes of the target variable, that is >>Default<< and >>Non-Default<<, thus the problem is a binary classification problem. 
    Logistic regression apply linear functions in the predictors, and since the outcome modeled is probabilities, they have to lie between $[0,1]$, several functions are available to ensure this, the logistic regression model uses the logistic function. 
    
    Let $Y$ be the dependent variable and $\boldsymbol{X} = \left(X_1, \dots, X_p\right)$ denote the predictor variables then we the logistic regression model is given by equation \ref{eq:logreg_1}:
    
    \begin{flalign} \label{eq:logreg_1}
    p_1\left( \boldsymbol{X} \right) = P \left( Y = 1 \mid \boldsymbol{X} \right) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_pX_p}}{1+e^{\beta_0 + \beta_1 X_1 + \dots + \beta_pX_p}} = \frac{e^{\beta_0 + \boldsymbol{\beta}^{\top} \boldsymbol{X}  }}{1 + e^{\beta_0 + \boldsymbol{\beta}^{\top} \boldsymbol{X^{\prime}} }}
    \end{flalign}
    
    $\beta_0$, $\boldsymbol{\beta}$ are the parameters of the model. The model applies the logistic function $f\left( x \right) = \frac{e^x}{1+e^x}$ to $\beta_0 + \boldsymbol{\beta}^{\top} \boldsymbol{X}$, the logistic function is depicted in Figure \ref{fig:logistic_function}, the figure exemplifies how the function is mapping values in the interval $\left[ -\infty, \infty \right]$ to always lie in the interval $\left[ 0,1 \right]$.
\begin{figure}[H]
    \centering
\begin{tikzpicture}
    \begin{axis}%
    [
        grid=minor,     
        xmin=-10,
        xmax=10,
        axis x line=bottom,
        ytick={0,.5,1},
        ymax=1,
        axis y line=middle,
        % style={ultra thick},
    ]
        \addplot%
        [
            AUdefault,%
            mark=none,
            samples=100,
            domain=-10:10,
            linewidth = 10pt,
        ]
        (x,{1/(1+exp(-x))});
    \end{axis}
\end{tikzpicture}
    \caption{The Logistic Function}
    \label{fig:logistic_function}
\end{figure}
The odds are given by equation \ref{eq:logreg_odds}

\begin{flalign} \label{eq:logreg_odds}
\frac{p_1\left( \boldsymbol{X} \right) }{1 - p_1\left( \boldsymbol{X} \right)} & = e^{\beta_0 + \boldsymbol{\beta^{\top} X}}
\end{flalign}

 the odds lies in the interval $\left[0, \infty \right]$. Values close to $0$ indicates a small probability of the event happening and the opposite of values close to $\infty$. Now take the logarithm of equation \ref{eq:logreg_odds} we obtain the logit in equation \ref{eq:logreg_logit}.

\begin{flalign} \label{eq:logreg_logit}
\log \left( \frac{p_1\left( \boldsymbol{X} \right) }{1 - p_1\left( \boldsymbol{X} \right)}\right) & = \beta_0 + \boldsymbol{\beta^{\top} X}
\end{flalign}

Equation \ref{eq:logreg_logit} is linear in parameters and explanatory variables. Thus by changing $X_i$, $i\in \{1, \dots, p \}$ by one unit, and assuming the other explanatory variables $\left(1 - p\right)$ is fixed, will yield a change in the logit by $\beta_i$. This however does not denote a change in $p_1\left(\boldsymbol{X}\right)$ by $\beta_i$, due to the nonlinear relationship between $p_1\left(\boldsymbol{X} \right)$ and $X_i$. It denotes that when $\beta_i > 0$ an increase in  $X_i$ does yield an increase in $p_1\left(\boldsymbol{X}\right)$ and vice versa. 

In order to estimate the logistic regression model denoted in Equation \ref{eq:logreg_1}, we rely on maximum likelihood method. The likelihood function in is depicted in Equation \ref{eq:logreg_LK}

\begin{flalign} \label{eq:logreg_LK}
\mathcal{L} \left(\beta_0, \boldsymbol{\beta} \right) & = \prod_{i:y_i=1} p_1\left(\boldsymbol{x}_i \right) \prod_{j:y_j=0} \left(1-p_1\left(\boldsymbol{x}_j \right)\right)
\end{flalign}    

in the likelihood function $y_i$ and $\boldsymbol{x_i}$ denotes the i'th observation in the dependent variable and explanatory variables. However we often work with the log-likelihood function instead of Equation \ref{eq:logreg_LK}, which is obtained by using Equation \ref{eq:logreg_odds} together with \ref{eq:logreg_logit} the resulting log-likelihood function is shown in Equation \ref{eq:logreg_LLK} below  

\begin{flalign}
l\left(\beta_0, \boldsymbol{\beta} \right) & = \log \left( \mathcal{L} \left(\beta_0, \boldsymbol{\beta} \right) \right) \nonumber \\
& = \sum_{i:y_i=1} \log \left(p_1 \left(\boldsymbol{X}_i \right) \right) + \sum_{j:y_j=0} \log\left( 1 - p_1\left( \boldsymbol{X}_j \right)\right) \nonumber\\
& = \sum_{i=1}^n \left(y_i \log \left( p_1 \left( \boldsymbol{X}_i \right) \right) + \left( 1-y_i \right) \log \left( 1 - p_1\left( \boldsymbol{X}_i \right)\right)\right) \nonumber \\
& = \sum_{i=1}^n \left(y_i \log \left(\frac{p_1\left( \boldsymbol{X}_i \right) }{1 - p_1\left( \boldsymbol{X}_i \right)} \right) + \log \left( 1 - p_1\left( \boldsymbol{X}_i 
\right)\right)\right) \nonumber \\
& = \sum_{i=1}^n \left(y_i \log \left(\frac{p_1\left( \boldsymbol{X}_i \right) }{1 - p_1\left( \boldsymbol{X}_i \right)} \right) + \log \left( \frac{p_1\left( \boldsymbol{X}_i \right) }{1 - p_1\left( \boldsymbol{X}_i \right)}\right) \right) \nonumber \\
& = \sum_{i=1}^n \left(y_i \left( \beta_0 + \boldsymbol{\beta}^{\top} \boldsymbol{X} \right) - \log \left( 1 + e^{1+ \beta_0 + \boldsymbol{\beta}^{\top} \boldsymbol{X} } \right) \right) \label{eq:logreg_LLK}
\end{flalign}

In Equation \ref{eq:logreg_LLK} $n$ denotes the number of observations in the data feed to the model. The model parameters that maximize the log-likelihood function above is denoted $\hat{\beta}_0$ and $\hat{\boldsymbol{\beta}}$, these estimates are then used when predicting the probabilities for an event happening, this event is in the case of the thesis the probability of a costumer defaulting on their mortgage. The estimated probabilities are obtained by using Equation  \ref{eq:logreg_estimates}

\begin{flalign}
\hat{p}_1 \left(\boldsymbol{X} \right) = \frac{e^{\hat{\beta}_0 + \hat{\boldsymbol{\beta}}^{\top}\boldsymbol{X}}}{1 + e^{\hat{\beta}_0 + \hat{\boldsymbol{\beta}}^{\top}\boldsymbol{X}}}. \label{eq:logreg_estimates}
\end{flalign}

    \section{Artificial Neural Networks}
    The groundwork of what later should become known as Artificial Neural Networks was conducted from the early 1940's and onwards. \cite{mcculloch43a} proposed a mathematical model for modelling the behavior of a biological neuron in the human brain, the proposed artificial neuron was capable of solving simple binary tasks, however it was not learning. Then \cite{hebb1949} suggested that the human brain learns by intensifying connections between neurons that "work together" and reduces the ones that does not. This postulate of how humans learns  led as an inspiration of the work of \cite{Rosenblatt1958:}, who proposed the Perceptron, illustrated in Figure \ref{fig:Perceptron_illustration}, this was the first ANN, which were able to adjust its own weights, that is finding the optimal weights in order to achieve the desired behavior of the Perceptron, in short it was learning. This was a short introduction to the beginning of the field of ANN, in the following we will further explain how the ANN is modelled and estimated.  
    \subsection{Perceptron}\label{subsec:Perceptron}
    The Perceptron introduced above, has a simple activation rule, which is the basis of modern ANNs. We have a n-dimensional input $\boldsymbol{X}=(x_1, \dots, x_n)$, the weighted sum of the n-dimensional input, $x_i$, together with its associated weights, $w_i$ and a bias term is computed as shown in equation \eqref{eq:Perceptron_1} below.
    \begin{figure}[h]
    \centering
    \begin{tikzpicture}[
init/.style={
  draw,
  circle,
  inner sep=2pt,
  font=\Huge,
  join = by -latex
},
squa/.style={
  draw,
  inner sep=2pt,
  font=\Large,
  join = by -latex
},
start chain=2,node distance=13mm
]
\node[on chain=2] 
  (x2) {\vdots};
\node[on chain=2,join=by o-latex] 
  {\vdots};
\node[on chain=2,init] (sigma) 
  {$\displaystyle\Sigma$};
\node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation \\ function}}]   
  {$f$};
\node[on chain=2,label=above:Output,join=by -latex] 
  {$y$};
\begin{scope}[start chain=1]
\node[on chain=1] at (0,1.5cm) 
  (x1) {$x_1$};
\node[on chain=1,join=by o-latex] 
  (w1) {$w_1$};
\end{scope}
\begin{scope}[start chain=3]
\node[on chain=3] at (0,-1.5cm) 
  (x3) {$x_n$};
\node[on chain=3,label=below:Weights,join=by o-latex] 
  (w3) {$w_n$};
\end{scope}
\node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

\draw[-latex] (w1) -- (sigma);
\draw[-latex] (w3) -- (sigma);
\draw[o-latex] (b) -- (sigma);

\draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
\end{tikzpicture}

    \caption{A single perceptron in a Artificial Neural Network. $x_n$ denotes the input variables and $w_n$ the associated weights, these are combined as $ \left(x^{(0)}_1 w^{(0)}_1 + \dots + x^{(0)}_n w^{(0)}_n + b_0\right)$, where $b_0$ is the bias term. This weighted average + bias is then sent to the affine transformation shown in equation \eqref{eq:Perceptron_2}.}
    
    \label{fig:Perceptron_illustration}
\end{figure}
    \begin{flalign} \label{eq:Perceptron_1}
    z & = \sum_{i=1}^n \left(w_i x_i \right) + b 
    \end{flalign}
    The weight, $w_i$, is often denoted as the \textit{preactivation}. In order to simplify notation the bias, $b$, is often substituted by an equivalent input term $x_0=1$, with the associated weight $w_0 = b$, which is the dot product of the weight vector $\boldsymbol{W}=\left(w_1, \dots, w_n \right)$ and the vector of inputs $\boldsymbol{X}=(x_1, \dots, x_n)$. $z$ is then send through an step function shown in equation \eqref{eq:Perceptron_2}, which calculates the output of the Perceptron, in this setting a step function. 
    \begin{flalign}\label{eq:Perceptron_2}
    y & = \begin{cases} 1, \qquad if z \ge 0, \\
                        0, \qquad otherwise
        \end{cases} 
    \end{flalign}
    Even though \citeauthor{Rosenblatt1958:}'s Perceptron is exciting, one even more exciting innovation from \cite{Rosenblatt1958:} is the algorithm, that made it possible for the model to find and adjust its own weights in order to solve the classification problem. The update algorithm take as given a training pair $(x, y)$, where $x$ is the input vector and $y$ is the associated output vector, then the parameters of the model (weights and bias) are learned through the rule presented in equation \eqref{eq:Perceptron_3}.
    \begin{flalign} \label{eq:Perceptron_3}
    w_i^{new} & = w_i^{old} - \eta \left(\hat{y} - y \right) x_i
    \end{flalign}
    in equation \eqref{eq:Perceptron_3} $\hat{y}$ is the proposed output of the Perceptron, $y$ is the target output, $x_i$ and $w_i$ is the i'th input and i'th weight at the latest iteration, and lastly $\eta$ is a scaling, which adjusts the scope of the modification in each iteration. 
    
    Even though the discovery of the Perceptron is very exciting it has its limitations. \cite{Min69} showed that a single layer Perceptron, was unable to solve non-linear separable problems. Multi-Layer Perceptrons (MLP) illustrated in Figure \ref{fig:NN1}, is a development of the Single Layer Perceptron, that introduce one or several intermediate layers between the input- and output layer, these intermediate layers are called hidden layers, in other words we are combining several Perceptrons. 
    
    The MLP is able of solving non-linear separable problems, since in every layer the pre-activation is succeeded by a non-linear transformation, that projects the input vector into a linearly separable space. However, even though the MLP was able of solving the non-linear problem, \cite{Min69} also showed that the update algorithm proposed by \cite{Rosenblatt1958:} was restricted to the single layer perceptrons and thus was unable to independently learn how to solve the non-linear problem. 
    Thus, another learning or updating algorithm was needed for this task. One such algorithm capable of updating the weights of an ANN with multiple hidden layers is the Backpropagation algorithm \cite{Rumelheart_1986}. The algorithm uses gradient descent for solving this task. We will take a closer look at the Backpropagation algorithm in section \ref{sec:NN_BP}, but now we will expand on the Multi-Layer Perceptron.
    
    \subsubsection{Multi-Layer Perceptron}
    Take a look at Figure \ref{fig:NN1}, which depicts a MLP Neural Network. We can think of the network in Figure \ref{fig:NN1} as expanding the Perceptron to have multiple layers between the input layer and the output layer, where the neurons in the intermediate layers takes the output of the neurons in the previous layer as input. Every link between the i'th neuron in the previous layer say $l-1$ to the j'the neuron in the current layer say $l$ has a weight associated to it say $w_{ij}^{l}$, and all the weights stored in a matrix, $\boldsymbol{W}^l$. As the case with the single layer Perceptron, every layer in the MLP setting computes an affine transformation (see equation \ref{eq:MLP_1}), which goes as input to a nonlinearity, in the single layer Perceptron we used a step function, in the MLP case we usually use a more smooth function, this function is commonly refereed to as an activation function. Commonly used activation function will be discussed in section \ref{sec:NN_activation}. 
    \begin{flalign}
    \boldsymbol{z}^{(l)} &= \boldsymbol{W}^{(l)} \boldsymbol{h}^{(l-1)}\label{eq:MLP_1} \\
    \boldsymbol{h}^{(l)} & = \sigma \left(\boldsymbol{z}^{(l)} \right) \label{eq:MLP_2}
    \end{flalign}
    Thus a single hidden layer in the MLP, can be thought of as a function that depends on some layer specific parameters, that is the weights $\boldsymbol{W^{(l)}}$, the bias $b^{(l)}$ and other parameters  as the number of neurons as well as choice of activation. Combining all these functions for each layer is resulting in the ouput vector $y$.
    \begin{figure}[h]
    \centering
    \begin{neuralnetwork}[height=8]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$\hat{y}_#2$}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
        \newcommand{\hthird}[2]{\small $h^{(3)}_#3$}
        \inputlayer[count=3, bias=true, title=Input\\layer, text=\x]
        \hiddenlayer[count=8, bias=false, title=Hidden\\layer 1, text=\hfirst]{\linklayers}
        \hiddenlayer[count=5, bias=false, title=Hidden\\layer 2, text=\hsecond] \linklayers
        \hiddenlayer[count=5, bias=false, title=Hidden\\layer 3, text=\hthird] \linklayers 
        \outputlayer[count=1, title=Output\\layer, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{Combining multiple perceptrons a Feedforward Multilayer Perceptron Neural Network can be obtained. The bias term is denoted $x_0$ and $h^{(h)}_n$ denotes a single neuron.}
    \label{fig:NN1}
\end{figure}
    \subsection{Activation Functions}\label{sec:NN_activation}
    A key component of the Neural Network is the activation function, which decides whether or not a neuron should be activated. The activation function is performing an element-wise nonlinear transformations of the pre-activations (the weights), from the affine transformation, this makes the Neural Network able to solve nonlineary separable problems. 
    
    The idea is that the affine transformation and the activation function are working together during the training of the neural network. The affine transformation, $\boldsymbol{z}^{(l)}$ is based on the weights, which are learned during the training, these are then sent through the activation function which maps it into another space, where they are more easily separable. 
    
    Many activation functions are available, some commonly used are the Logistic, ReLU and tanh, these are depicted in Figure \ref{fig:activations}, where we visually denotes the difference between them. In the following we will shortly introduce the three activation functions.

        \begin{figure}[t]
    \centering
    \begin{tikzpicture}[domain=-2.9:2.9]
        % grid
        \draw[very thin,color=gray] (-2.9, -1.1) grid (2.9, 2.9);
        \foreach \x in {-2,-1,0,1,2}
            \node[anchor=north] at (\x,-1.1) {\x};
        \foreach \y in {-1,0,1,2}
            \node[anchor=east] at (-2.9,\y) {\y};
        % axes
        \draw[->,thick] (-2.9, 0) -- (3.1, 0) node[right] {$x$};
        \draw[->,thick] (0, -1.1) -- (0, 3.1) node[above] {$f(x)$};

        % Logistic
        \draw[thick, color=black]
            plot (\x, { 1/(1+exp(-1 * \x)) })
            node[right,yshift=-0.5em] {$logistic(x)$};
        % Beta logistic
        %\draw[color=purple] plot (\x, { 1/(1+exp(-.5 * \x)) })
        %    node[right] {$logistic(a), beta=.5$};
        % Tanh
        \draw[thick, color=orange]
            plot (\x, { ( 1 - exp(-2*\x) ) / ( 1 + exp(-2*\x) ) })
            node[right,yshift=0.5em] {$tanh(x)$};
        % ReLU
        \draw[thick, color=AUdefault]
            plot (\x, { max(0, \x) })
            %node[right] {$ReLU(x)$};
            node[yshift=1em] (a) {$ReLU(x)$};
        
    \end{tikzpicture}
    \caption{Common activation functions: Logistic, tanh and ReLU.\label{fig:activations}}
\end{figure}
            \subsubsection{Logistic}
            The Logistic function also denoted Sigmoid, is a function that takes a number and maps it to be between 0 and 1. It is defined as in equation \eqref{eq:AF_Logistic_1} and depicted in Figure \ref{fig:activations} as the black line. The presentation in the figure visualises the fact that large negative numbers maps to 0, and large positives numbers maps to 1.   
            \begin{flalign}\label{eq:AF_Logistic_1}
            \sigma \left( \boldsymbol{z} \right) &= \frac{1}{1 + e^{\boldsymbold{z}}}
            \end{flalign}
            An important quality of the Logistic function is that when we have solved to derivative analitically, it is very fast to compute. We have obtained the derivative below in equation \eqref{eq:AF_Logistic_2}.
            \begin{flalign}
            \frac{\partial \sigma \left( \boldsymbold{z} \right)}{\partial \boldsymbold{z}} & = \frac{e (-\mathbf{z})}{(1+e ^{(-\mathbf{z}))^{2}}} \nonumber\\
            \frac{\partial}{\partial \mathbf{z}} \sigma(\mathbf{z}) &=\frac{1}{1+e^{ (-\mathbf{z})}}  \frac{e^{ (-\mathbf{z})}}{1+e^{ (-\mathbf{z})}} \nonumber\\
            &=\sigma(\mathbf{z})  \frac{e^{ (-\mathbf{z})}}{1+e ^{(-\mathbf{z})}} \nonumber\\
            &=\sigma(\mathbf{z})  \frac{1+e ^{(-\mathbf{z})}-1}{1+e ^{(-\mathbf{z})}}\nonumber \\
            &=\sigma(\mathbf{z}) \left(1-\frac{1}{1+e^{(-\mathbf{z})}}\right) \nonumber\\
            &=\sigma(\mathbf{z}) (1-\sigma(\mathbf{z})) \label{eq:AF_Logistic_2}
            \end{flalign}
            The Logistic function have been very widely used in the past however, it has two major drawbacks. 
            
            The first being the vanishing gradient problem and is explained by \cite{Zhang2015}. The problem is that since the Backpropagation algorithm depends on the value of the derivative when updating the weights of the network this combined with the logistic function saturating at large values results in a slow learning process and in some cases prevents it. 
            
            The second drawback is that the output is not zero-centered, which permits neurons in later layers to either take all positive or negative inputs. This can result in a sign switching behaviour between the layers in the network, slowing down the learning process \cite{Zhang2015}.
            \subsubsection{Tanh}
            The hyberbolic tangent (tanh) activation function maps values to being in the interval $[-1, 1]$. Tanh also suffers, as the Logistic function, from the vanishing descent problem, however the mean is zero-centered, which is positive since negative (positive) inputs will be mapped in the negative (positive) direction \cite{godina_2017}. As seen from the definition in equation \eqref{eq:AF_Tahn}, tanh uses the exponential function, which is expensive to compute compared to for example the ReLU activation function.
            \begin{flalign}\label{eq:AF_Tahn}
            \sigma \left( \mathbf{z} \right) &= \frac{1 - e ^{-2 \mathbf{z}}}{1 + e ^{-2 \mathbf{z}}}
            \end{flalign}
            \subsubsection{ReLU}
            The idea of the Rectified Linear Unit (ReLU) is a very simple yet the most used activation function at the time, it is mapping input values to $[0,\infty]$, see definition in equation \eqref{eq:AF_ReLU}. The simplicity of the ReLU activation function makes it very efficient to compute, when comparing to the Logistic- and Tanh activation functions. Even though the function is very simple it has some nice properties and actually only few drawbacks. 
            The ReLU is 0 for negative values, but does not saturate for positive values  \cite{Zhang2015}, which ensures a flow of gradient as long as the input is positive, this have been found to increase the speed to convergence when training the network. 
            \begin{flalign}\label{eq:AF_ReLU}
            \sigma \left( \mathbf{z} \right) &= \operatorname{max} \left(0, \mathbf{z} \right)
            \end{flalign}
            
            \section{Back-propagation}\label{sec:NN_BP}
            As mentioned earlier in subsection \ref{subsec:Perceptron}, the learning algorithm developed by \cite{Rosenblatt1958:} is unable to learn in the case where the Perceptron has multiple layers, such as the network presented in Figure \ref{fig:NN1}. The problem is that in order to update the weights and biases equation \eqref{eq:Perceptron_3}, requires knowledge of the true output vector, which in the MLP case only is available in the output layer.
            
            \cite{Rumelheart_1986} noted  that the calculation performed by the activation function is a nonlinear, and differentiable function, which made it possible to compute the partial derivatives of the error with respect to the weights of the ANN. Meaning we can use calculus to calculate the magnitude by which each neuron in the output layer contributes to the error and then divide the contribution to the error of each neuron in the preceding layers. In this way the error is >>propagated backwards<< through the ANN, adjusting the weights accordingly to the contribution to the error at the output layer.  
            
            The algorithm is then optimizing the parameters iteratively through a two-step procedure which are made of a forward- and a backward pass. 
            Considering Figure \ref{fig:NN1}, the forward pass refers to the computation procedure through all the networks neurons from the input layer to the output layer, which produce the prediction $\hat{y}$. 
            The backward pass is where the parameter (weight and bias) is recalculated and thus updated. This update of the parameters is based on the error $(E=\hat{y} - y)$ produced by the network in the forward pass. 
            
The purpose of the algorithm is to update all the weights and biases through the ANN, this is accomplished by computing the partial derivative of a loss function with respect to each weight and bias in the ANN. The choice of loss function depends on the problem being adressed, in a classification problem we will typically apply the cross-entropy loss (see equation \eqref{eq:cross-entropy}), independent of the choice of loss function the same principles applies, namely that the updating algorithm seek to adjust the parameters in order to minimize an error \cite{ESL}.
            
More formally we update the weights according to equation \eqref{eq:weight_update_BP}
    
            \begin{flalign}
            \mathbf{W}_{new} & = \mathbf{W}_{old} - \eta \frac{\partial E}{\partial \mathbf{W}_{old}} \label{eq:weight_update_BP}
            \end{flalign}
here $\eta$ is the so called learning rate which indicates the step size of the gradient descent steps.

\subsubsection{Cross-Entropy loss function}
The Cross-Entropy loss function also known as the log-loss, is as mentioned above usually applied in classification tasks. The loss is computed as in equation \eqref{eq:cross-entropy}

\begin{flalign}
\mathcal{L}\left(y, \hat{y} \right) &= - \sum_i y_i \log \hat{y}_i \label{eq:cross-entropy}
\end{flalign}

where $y$ is the true value of the output and $\hat{y}_i$ is the model estimate of the output. The cross-entropy loss function introduces an exponentially increasing error as the difference of the output estimate and true output increases and vice versa.  


\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Write here the result }

 initialization\;
 \For{d in data}{
 \textbf{Forward Pass:}
  \begin{itemize}
      \item Starting from the input layer, use eq.~\ref{update} to do
      a forward pass trough the network, computing the activities of the
      neurons at each layer.
  \end{itemize}
        \textbf{Backward Pass}\;
        \For{$\ell$ in $\mathcal{L}$}{
        \begin{itemize}
            \item Compute the derivatives of the error function with respect
        to the inputs of the upper layer neurons\;
            \item Compute the derivatives of the error function with respect
        to the weights between the outer layer and the layer below\;
            \item Compute the derivatives of the error function with respect
        to the activities of the layer below\;
        \end{itemize}
        } 
        Update weights\;
 }
 \caption{Back-propagation}
\end{algorithm}


\section{Performance measures}
Now that we introduced the Logistic Regression and the Artificial Neural Network, we will consider how to evaluate the performance of the two classifiers. This chapter is based on \cite[pp. 146 - 149]{ISL}. 

As mentioned throughout the thesis we want to predict whether or not a loan is going in the state of default or not, by using the explanatory variables. That is, predicting a binary target variable, $Y$ using explanatory variables $\boldsymbold{X}$. As described in the sections above, we do not directly model $Y$, instead we model the probability of $Y$ taking the value 1 (default), $p_1\left( \boldsymbol{X} \right) = P \left( Y = 1 \mid \boldsymbol{X} \right)$, thus also obtaining a level of confidence in the prediction. 

When such a probability of default model has been obtained, $Y$ can be predicted with the explanatory variables $\boldsymbold{X}$ by using 

\begin{flalign}\label{eq:performance_1}
\hat{Y} & = \begin{cases}1, \qquad \ if p_1\left(\boldsymbold{X} \right)  > \alpha\\ 0, \qquad if \ p_1\left(\boldsymbold{X} \right) \leq \alpha \end{cases}
\end{flalign}

where $\alpha \in [0,1]$ is the threshold. Now that we have obtained predictions for $\gamme$, we would like to evaluate the performance of our models. Suppose we have a dataset with n observation $\left\{ \left(y_1, \boldsymbold{x}_1\right), \dots, \left(y_n, \boldsymbold{x}_n \right) \right\}$, and let $\left\{\hat{y}_1, \dots, \hat{y}_n \right\}$ be our predictions for a given threshold, $\alpha$. 

One simple and intuitive performance measure is the misclassification rate defined as 
\begin{flalign}\label{eq:performance_2}
err = \frac{\sum_{i=1}^{n} \mathbbm{1} \left(y_i \neq \hat{y}_i \right)}{n}
\end{flalign}
which simply is the proportion of misclassified observations. Alternatively the accuracy is defined as
\begin{flalign}\label{eq_performance_3}
acc & = 1 - err 
\end{flalign}
which is the proportion of correctly classified observations. Both of these measure can be misleadning when the classes of the target variable is very unbalanced, meaning one class is over represented in the data. In our data approximately 2.8\% belongs to the class default in the data. The reason why these metrics can be misleading in the case of class imbalance, is that we easily is able to obtain a high accuracy and similar very low misclassification rate simply by assigning every prediction to the majority class. In our case we could obtain an accuracy of 97.2\%, by assigning all predictions to the class >>non-default<<. Thus, a model like this would be unusable and very misleading in the risk management point of view.

To illustrate why, note out classifiers is able of producing two types of erros:
\begin{itemize}
    \item Type I Error: False Positive (FP)
    \item Type II Error: False Negative (FN).
\end{itemize}

Thus we can incorrectly assign a observation which does not default to the default class, that is the type I error, or again incorrectly assign an observation who actually defaults to the class non-default, which is the type II error.

The confusion matrix depicted in Figure \ref{tab:Confusion_matrix}, is a practical view of the Type I and II error allocation.
\begin{table}[H]
    \centering
        \caption{{Confusion Matrix. The diagonal represents observations, that were correctly classified. The off-diagonal represents observations that were misclassified.}}
    \label{tab:Confusion_matrix}
\begin{tabular}{@{}cc cc@{}}
\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{2}{c}{\textbf{Actual}} \\ 
\cmidrule(lr){3-4}
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{Negative} & 
\multicolumn{1}{c}{Positive} \\ 
\vspace{0.1cm}
\cline{2-4}
\multirow[c]{2}{*}{\rotatebox[origin=tr]{90}{\textbf{Predicted}}}
& Negative  & True Negative (TN) & False Positive (FP)  \\[1.5ex]
& Positive  & False Negative (FN)  & True Positive (TP) \\
\cline{2-4}
\end{tabular}
\end{table}

While the confusion matrix in itself provides useful information, and we know we would like to have high numbers on the diagonal and small numbers on the off-diagonal it rapidly becomes confusing to interpret the matrix hereof the name. Thus, we use the matrix to calculate some performance metrics, that are easier to interpret. Some of these are presented below.

\begin{flalign}
Sensitivity & =  \frac{TP}{TP + FN} \label{eq:sensitivity}\\
Specificity & = \frac{TN}{TN + FP} \label{eq:specificity} \\
PPV & = \frac{TP}{TP + FP} \label{eq:PPV}\\
NPV & = \frac{TN}{TN + FN}  \label{eq:NPV} \\ 
Accuracy & = \frac{TP + TN}{TP + TN + FP + FN} \label{eq:acc}
\end{flalign}

 Sensitivity represents the rate of which the classifier was able to predict true classes out of all the true default cases. Specificity represents the proportion of true negative (non-default) cases that the classifier correctly classified. PPV (Positive Predictive Value or Precision) and NPV (Negative Predictive Value) represents the proportion of true positive (negative) cases out of all positive (negative) cases. Accuracy describes the overall performance of the classifier all correctly classified cases out of all the cases.
 
 When considering the default classification problem from the point of view of a bank, misclassifying a costumer who truly are not defaulting translates into loss of potential profit. On the other hand misclassifying a true default will result in a potential loss from the costumer not repaying what is owed to the bank. 
 
 In this setting the bank has to decide which of the two risks it is more concerned with or whether it is equally concerned with both. Recall Equation \eqref{eq:performance_1}, an $\alpha$ of 0.5 indicates that the bank equally weighs the two risks. By reducing the threshold, $\alpha$, in the classification rule, we will detect more defaults as the probability of a costumer defaulting now doesn't need to be above 0.5, this however is on compromise of misclassifying non-defaulters as defaults. One could imagine that a bank rather would miss out on profit than taking a loss on an existing costumer because it did not detect the costumer was going to default. 
 
 Instead of taking a subjective decision about the risk aversion by choosing a specific $\alpha$ we introduce another metric which is based on the Receiver Operating Characteristics (ROC) curve. The ROC curve is the outcome of plotting $(1-specificity(\alpha))$ against $sensitivity(\alpha)$ for all values of $\alpha \in [0,1]$. Thus, illustrating the trade-off between specificity and sensitivity.
 
 We would like the ROC curve to be as close as possible to the upper-left corner of the graph. That is, having the largest possible value (near 1) of $sensitivity(\alpha)$ and the lowest possible value (near 0) of $(1-specificity(\alpha))$. 
 Comparing ROC curves of two competing models, the model with a ROC curve strictly above the other is the better model since it yields a higher sensitivity for any given $(1-specificity(\alpha))$.

 \section{Local Interpretable Model-agnostic Explanations (LIME)}

Artificial Neural Networks have been and are often strongly critized due to their complexity. If one were told to interpret an ANN, the natural thing would be to look at the parameters (weights and bias), if the ANN only had a input layer and output layer we would have as many parameters as we have explanatory variables plus a bias term, and the parameters are connected to a specific variable, and thus can be interpreted as an effect of that specific variable on the target variable. However as we add nodes and hidden layers to our ANN, the number of parameters in the network increases rapidly. An example where we have 30 explanatory variables 3 hidden layers where each layer has 128 nodes then we end up with 

\begin{flalign}
P & = \underbrace{30 \times 128}_{input} + \overbrace{128 \times 128 + 128 \times 128}^{hidden} + \underbrace{128 \times 1}_{output} + \overbrace{128 + 128 + 128 + 1}^{biasses} = 31.721 \nonumber
\end{flalign}

thus the interpretability task becomes quite infeasible. If we insist on utilizing ANN for deciding whether or not to grant a loan to a costumer, then according to regulation we need the ability to tell the costumer exactly what caused the result, for instance if it was the costumers income to debt ratio that were too high. 
\cite{lime_2016} propose a simple method for opening these >>black-box<< models, by approximating a local interpretable model around each observation...

If we want to use an ANN for making decisions on which costumers we want to  costumers we are required by regulation to being able to tell exactly why we are rejecting a costumer. That is why we will explain a new technique proposed in \cite{lime_2016} which is a simple approach of unboxing the black-box classifers in machine learning.