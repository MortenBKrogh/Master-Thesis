\chapter{Methodology}

    \section{Artificial Neural Networks}

\begin{figure}[H]
    \caption{Neural Network Visualized}
    \centering
    \label{fig:my_label}
\begin{neuralnetwork}[height=12]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$\hat{y}_#2$}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
        \newcommand{\hthird}[2]{\small $h^{(3)}_#3$}
        \inputlayer[count=3, bias=true, title=Input\\layer, text=\x]
        \hiddenlayer[count=8, bias=false, title=Hidden\\layer 1, text=\hfirst] \linklayers
        \hiddenlayer[count=12, bias=false, title=Hidden\\layer 2, text=\hsecond] \linklayers
        \hiddenlayer[count=12, bias=false, title=Hidden\\layer 3, text=\hthird] \linklayers 
        \outputlayer[count=1, title=Output\\layer, text=\y] \linklayers
    \end{neuralnetwork}
\end{figure}
\lipsum[1-5]

        \subsection{Feed forward Neural Networks}

        \subsection{Activation Functions}

            \subsubsection{Sigmoid}

            \subsubsection{Tanh}
            
            \subsubsection{ReLU}
            
        \subsection{Loss Functions}
            
            \subsubsection{Cross-Entropy Loss}
        
        \subsection{Training Neural Networks}
            
            \subsubsection{Gradient Descent}
            
            \subsubsection{Mini-Batch Gradient Descent}
            
            \subsubsection{Learning Rate}
            
            \subsubsection{Momentum}
            
            \subsubsection{Backpropagation}
            
            \subsubsection{Weight Initialisation}
            
            \lipsum[1-2]
    
        \subsection{Layers}
            
            \subsection{Batch Normalisation}
            
        \subsection{Regularisation}
        
            \subsubsection{Dropout}
            
            \subsubsection{$L^2$ Regularisation}
            
        
